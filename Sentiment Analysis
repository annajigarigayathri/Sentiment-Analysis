import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk import classify
from nltk import NaiveBayesClassifier
import random
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
def preprocess_text(text):
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return tokens
def extract_features(text):
    words = preprocess_text(text)
    return dict([(word, True) for word in words])
positive_reviews = [("I love this restaurant", "positive"), ("The food was amazing", "positive"), ("Great service!", "positive")]
negative_reviews = [("Terrible experience", "negative"), ("Disappointing food", "negative"), ("Poor service", "negative")]
labeled_data = [(extract_features(review), sentiment) for review, sentiment in positive_reviews + negative_reviews]
random.shuffle(labeled_data)
train_set = labeled_data[:int(0.8*len(labeled_data))]
test_set = labeled_data[int(0.8*len(labeled_data)):]
classifier = NaiveBayesClassifier.train(train_set)
accuracy = classify.accuracy(classifier, test_set)
print("Accuracy:", accuracy)
input_text = input("Enter the text for sentiment analysis: ")
features = extract_features(input_text)
sentiment = classifier.classify(features)
print("Sentiment of '{}' is: {}".format(input_text, sentiment))
